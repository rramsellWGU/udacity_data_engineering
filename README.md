# Data Modeling with PostgreSQL
________________________________________________________________________________________________________________________________
________________________________________________________________________________________________________________________________


### Project Introduction
________________________________________________________________________________________________________________________________


Sparkify, a start-up company, wants to analyze the data they have been collecting on songs and user activity on their music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The business need is for a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis. The goal is to create a database schema and ETL pipeline for this analysis. 

### How to Run Python ETL Scripts
________________________________________________________________________________________________________________________________


1. Download Python 3 and have the below packages installed for import:
    os, glob, psycopg2, pandas, datetime, sql_queries
    
2. In the terminal run the following statements in the specified order:

>-> python create_tables.py

>-> python etl.py

### Description of Files in Repository
________________________________________________________________________________________________________________________________


##### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. Below is an example of the partitioning and json object formatting.

>song_data/A/B/C/TRAEXAMPLES.json

>song_data/A/A/B/TRAEXAMPLES.json

>{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

##### Logs Dataset

The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate activity logs from the music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month. 

>log_data/2018/11/2018-11-12-events.json

>log_data/2018/11/2018-11-13-events.json

### Database Schema Design
________________________________________________________________________________________________________________________________

The star schema contains the tables: songplays, songs, artists, users, and time. Below is a breakdown of Entity Relationships ER within the schema.

##### Songplays
- songplay_id serial **primary key**
- start_time timestamp
- user_id bigint **foreign key**
- level varchar
- song_id varchar **foreign key**
- artist_id varchar **foreign key**
- session_id varchar
- location varchar 
- user_agent varchar

##### Songs
- song_id varchar **primary key**
- title varchar
- artist_id varchar **foreign key**
- year int
- duration decimal(18,5)

##### Artists
- artist_id varchar **primary key**
- name varchar
- location varchar
- latitude double precision
- longitude double

##### Users
- user_id bigint **primary key**
- first_name varchar
- last_name varchar
- gender varchar
- level varchar

##### Time
- start_time timestamp **primary key***
- hour int
- day int
- week int
- month int
- year int
- weekday varchar

### ETL Pipeline and Description of Files
________________________________________________________________________________________________________________________________

The below files together create the ingestion of logs and song data. Attached is a description.

##### create_tables.py
This file drops and creates both the sparkify database as well as each table in the schema (should be run first).

##### sql_queries.py
This file contains the queries which create the tables in the schema as well as their insert statements and drop statements.

##### etl.ipynb
This is a scratchpad which was used to understand the raw data and how to best interact with it upon ingestion. In this file is the logical justification for the entities and their relationships.

##### etl.py
This is a simplified and streamlined version of the etl.ipynb intended for ingestion in production. A final draft so to speak. 

##### test.ipynb
This file is a QA on the etl.py run. It ought to be run at the completion of each ingestion to check for issues.
________________________________________________________________________________________________________________________________
